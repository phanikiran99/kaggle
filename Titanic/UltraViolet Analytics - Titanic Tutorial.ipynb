{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UltraViolet Analytics - Titanic Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is my 3rd submission , after failing with basic models using graphlab and sklearn \n",
    "#### This time as a beginer i opted to visit the tutorials available to make me walk through the data set <br>\n",
    "<a href=http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/> Tutorial Available Here </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-1 , Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 columns: ['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n",
      "Row count: 1309\n"
     ]
    }
   ],
   "source": [
    "# read the training and test datasets \n",
    "titanic_train_df = pd.read_csv('train.csv', header=0)\n",
    "titanic_test_df = pd.read_csv('test.csv', header=0)\n",
    "\n",
    "# Merge both datasets into one dataset so we will have more training data\n",
    "titanic_df = pd.concat([titanic_train_df, titanic_test_df])\n",
    "\n",
    "# re-numbering the dataset so tht there are no duplicates\n",
    "titanic_df.reset_index(inplace=True)\n",
    "\n",
    "# reseting the index will create new column which we dont want so lets drop it\n",
    "titanic_df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "# re-index the axis so first element will have index as 0\n",
    "titanic_df = titanic_df.reindex_axis(titanic_train_df.columns, axis=1)\n",
    "\n",
    "print titanic_df.shape[1], \"columns:\", titanic_df.columns.values\n",
    "print \"Row count:\", titanic_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1046 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 122.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the data is available in df\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-2, Working with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obviously many methods to work with missing values, we can either drop the records which have misisng values or we can fill up with some meaningful data like mean/median etc..the first one stands as good approach when we have lots of missing data for a particular column, as we cant just fill the data based on few data points available. Where are the second one is best approach in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign a value indicating that the value is missing.\n",
    "\n",
    "# we see that the cabin has lots of missing values, so it is good approach to fill the cabin details with missing details\n",
    "titanic_df['Cabin'] = titanic_df['Cabin'].fillna('U0')\n",
    "# So where ever we have the cabin details as U0 we can think it as missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Assing Average Values or most used values \n",
    "\n",
    "# we can assign this data to fare/price of ticket and also port of Embarked\n",
    "# Adjusting price \n",
    "titanic_df['Fare'] = titanic_df['Fare'].fillna(titanic_df['Fare'].median())\n",
    "# Above statement can also be written as below\n",
    "# titanic_df['Fare][np.isnan(titanic_df['Fare'])] = titanic_df['Fare'].median()\n",
    "\n",
    "# Adjusting Embarked\n",
    "titanic_df['Embarked'][titanic_df.Embarked.isnull()] = titanic_df['Embarked'].dropna().mode().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1046 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1309 non-null float64\n",
      "Cabin          1309 non-null object\n",
      "Embarked       1309 non-null object\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 122.8+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the values for Age missing , for entries like these we can apply multiple methods but a better approach would be to train on regression model to predict the missing values.<br>\n",
    "We will use RandomForestRegression from sklearn.ensemble to fill out the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setMissingAge(df):\n",
    "    \n",
    "    # features that we want to be part of regression \n",
    "    age_df = df[['Age','Embarked','Fare','Parch','SibSP','Title_id','PClass','Names','CabinLetter']]\n",
    "    \n",
    "    # Split age into two based on data available and data missing\n",
    "    knownAge = age_df.loc[ (df.age.notnull()) ]\n",
    "    unknownAge = age_df.loc[ (df.age.isnull()) ]\n",
    "    \n",
    "    # target column or y \n",
    "    y = knownAge.values[:,0]  # All rows and first column \n",
    "    \n",
    "    # train data or X \n",
    "    X = knownAge.values[:, 1::]\n",
    "    \n",
    "    # Create and fit a model\n",
    "    rtr_model = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "    rtr_model.fit(X, y)\n",
    "    \n",
    "    # use model to predict the values for age\n",
    "    predictedAges = rtr_model.predict(unknownAge.values[:,1::])\n",
    "    \n",
    "    # Assign values back to original dataset\n",
    "    df.loc[(df.Age.isnull()), 'Age'] = predictedAges\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-3, Feature Engineering, Variable Transformation\n",
    "Scikit-learn requires everything to be numeric so we’ll have to do some work to transform the raw data.<br>\n",
    "\n",
    "Different types of transformations can be applied to different types of variables. Qualitative transformations include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dummy Variables, this method is usefull when we have less number of values for a selected column, \n",
    "# For embarked we can see that only values that are available are 'S' 'C' 'Q' also as for sklearn \n",
    "# we need all the values to be converted to numeric rather than string/text data\n",
    "dummies_df = pd.get_dummies(titanic_df['Embarked'])\n",
    "dummies_df = dummies_df.rename(columns = lambda x: 'Embarked_' + str(x))\n",
    "\n",
    "titanic_df = pd.concat([titanic_df,dummies_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Factorizing - Pandas has a method called factorize() that creates a numerical categorical variable from any other variable, \n",
    "# assigning a unique ID to each distinct value encountered.\n",
    "# This method is used for alphanumeric data \n",
    "# we can apply this for Cabin data where we have alphanumeric.\n",
    "\n",
    "titanic_df['Cabin'][titanic_df.Cabin.isnull()]  = 'U0' # already did this above, but simply redoing with no purpose \n",
    "\n",
    "# Finds all First charecters of Cabin and groups it\n",
    "titanic_df['CabinLetter'] = titanic_df['Cabin'].map(lambda x: re.compile(\"[a-zA-Z]+\").search(x).group())\n",
    "\n",
    "# Convert all CabinLetter values to incremenetal integers using Factorize\n",
    "titanic_df['CabinLetter'] = pd.factorize(titanic_df['CabinLetter'])[0]  #Index 1 contains index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative Transormations Include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scaling : Scaling is a technique used to address an issue with some models that \n",
    "# variables with wildly different scales will be treated in proportion to the magnitude of their values. \n",
    "\n",
    "# standardscalar will subtract the mean from each value and then scale to unit variance\n",
    "# We will apply this method later once we have the NaN Values from age get filled up\n",
    "# scalar = preprocessing.StandardScaler()\n",
    "# titanic_df['Age_Scaled'] = scalar.fit_transform(titanic_df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binning : Binning is a method to distribute the data into equal bins\n",
    "# Lets Apply this method to Fare \n",
    "\n",
    "titanic_df['Fare_bin'] = pd.qcut(titanic_df['Fare'], 4) \n",
    "# qcut creates a new variable that identifies the quartile range \n",
    "\n",
    "# Factorize to create dummies from the result\n",
    "titanic_df['Fare_bin_id'] = pd.factorize(titanic_df['Fare_bin'])[0]\n",
    "# Finally we will get fare_bin_id for each fare from 0,1,2,3 values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-4, Feature Engineering : Derived Variables\n",
    " - Any variable that is generated from one or more existing variables is called a “derived” variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Get useful information from the Name\n",
    "\n",
    "# findout how many names are there for each person\n",
    "titanic_df['Names'] = titanic_df['Name'].map(lambda x: len(re.split(\" \", x)))\n",
    "\n",
    "# Now findout title of each person \n",
    "titanic_df['Title'] = titanic_df['Name'].map(lambda x: re.compile(\",(.*?)\\.\").findall(x)[0])\n",
    "\n",
    "# Let us fix rare occuring Titles\n",
    "titanic_df['Title'][titanic_df.Title == 'Jonkheer'] = 'Master'\n",
    "titanic_df['Title'][titanic_df.Title.isin(['Ms','Mlle'])] = 'Miss'\n",
    "titanic_df['Title'][titanic_df.Title == 'Mme'] = 'Mrs'\n",
    "titanic_df['Title'][titanic_df.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n",
    "titanic_df['Title'][titanic_df.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n",
    "\n",
    "# Build Binary Features\n",
    "titanic_df = pd.concat([titanic_df,pd.get_dummies(titanic_df['Title']).rename(columns=lambda x: 'Title_' + str(x))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix the cabin\n",
    "# Create a feature for the deck\n",
    "titanic_df['Deck'] = titanic_df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "titanic_df['Deck'] = pd.factorize(titanic_df['Deck'])[0]\n",
    "\n",
    "# Create binary features for each deck\n",
    "decks = pd.get_dummies(titanic_df['Deck']).rename(columns=lambda x: 'Deck_' + str(x))\n",
    "titanic_df = pd.concat([titanic_df, decks], axis=1)\n",
    "\n",
    "# Create feature for the room number\n",
    "# titanic_df['Room'] = titanic_df['Cabin'].map( lambda x : re.compile(\"([0-9]+)\").search(x).group()).astype(int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processTicket():\n",
    "    \n",
    "    global titanic_df\n",
    "    \n",
    "    # extract and massage ticet prefix\n",
    "    df['TicketPrefix'] = df['Ticket'].map( lambda x : getTicketPrefix(x.upper()))\n",
    "    df['TicketPrefix'] = df['TicketPrefix'].map( lambda x: re.sub('[\\.?\\/?]', '', x) )\n",
    "    df['TicketPrefix'] = df['TicketPrefix'].map( lambda x: re.sub('STON', 'SOTON', x) )\n",
    "    # create binary features for each prefix\n",
    "    prefixes = pd.get_dummies(df['TicketPrefix']).rename(columns=lambda x: 'TicketPrefix_' + str(x))\n",
    "    df = pd.concat([df, prefixes], axis=1)\n",
    "    \n",
    "    # factorize the prefix to create a numerical categorical variable\n",
    "    df['TicketPrefixId'] = pd.factorize(df['TicketPrefix'])[0]\n",
    "    \n",
    "    # extract the ticket number\n",
    "    df['TicketNumber'] = df['Ticket'].map( lambda x: getTicketNumber(x) )\n",
    "    \n",
    "    # create a feature for the number of digits in the ticket number\n",
    "    df['TicketNumberDigits'] = df['TicketNumber'].map( lambda x: len(x) ).astype(np.int)\n",
    "    \n",
    "    # create a feature for the starting number of the ticket number\n",
    "    df['TicketNumberStart'] = df['TicketNumber'].map( lambda x: x[0:1] ).astype(np.int)\n",
    "    \n",
    "    # The prefix and (probably) number themselves aren't useful\n",
    "    df.drop(['TicketPrefix', 'TicketNumber'], axis=1, inplace=True)\n",
    "    \n",
    " \n",
    "def getTicketPrefix(ticket):\n",
    "    match = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'U'\n",
    " \n",
    "def getTicketNumber(ticket):\n",
    "    match = re.compile(\"([\\d]+$)\").search(ticket)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 , Feature Engineering - Interaction Variables and Corellation \n",
    "Interaction variables capture effects of the relationship between variables. They are constructed by performing mathematical operations on sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled', 'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-15949e7c694e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m numerics = df.loc[:, ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled', \n\u001b[1;32m----> 2\u001b[1;33m                       'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# for each pair of variables, determine which mathmatical operators to use based on redundancy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumerics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;31m# ugly hack for GH #836\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m                 raise ValueError(\"Location based indexing can only have [%s] \"\n\u001b[0;32m    140\u001b[0m                                  \"types\" % self._valid_types)\n",
      "\u001b[1;32mC:\\Users\\IBM_ADMIN\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 raise KeyError(\"None of [%s] are in the [%s]\" %\n\u001b[1;32m-> 1367\u001b[1;33m                                (key, self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled', 'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']] are in the [columns]\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
